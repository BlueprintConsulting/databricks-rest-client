/*
 * Jobs API 2.1
 * The Jobs API allows you to create, edit, and delete jobs.
 *
 * The version of the OpenAPI document: 2.1
 *
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package com.edmunds.rest.databricks.DTO.jobs;

import com.edmunds.rest.databricks.DTO.clusters.AutoScaleDTO;
import com.edmunds.rest.databricks.DTO.clusters.AwsAttributesDTO;
import com.edmunds.rest.databricks.DTO.clusters.AzureAttributes;
import com.edmunds.rest.databricks.DTO.clusters.ClusterLogConfDTO;
import com.edmunds.rest.databricks.DTO.clusters.InitScriptInfoDTO;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonPropertyOrder;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Objects;

/**
 * NewClusterDTO
 */
@JsonPropertyOrder({
        NewClusterDTO.JSON_PROPERTY_NUM_WORKERS,
        NewClusterDTO.JSON_PROPERTY_AUTOSCALE,
        NewClusterDTO.JSON_PROPERTY_SPARK_VERSION,
        NewClusterDTO.JSON_PROPERTY_SPARK_CONF,
        NewClusterDTO.JSON_PROPERTY_AZURE_ATTRIBUTES,
        NewClusterDTO.JSON_PROPERTY_NODE_TYPE_ID,
        NewClusterDTO.JSON_PROPERTY_DRIVER_NODE_TYPE_ID,
        NewClusterDTO.JSON_PROPERTY_CUSTOM_TAGS,
        NewClusterDTO.JSON_PROPERTY_CLUSTER_LOG_CONF,
        NewClusterDTO.JSON_PROPERTY_INIT_SCRIPTS,
        NewClusterDTO.JSON_PROPERTY_SPARK_ENV_VARS,
        NewClusterDTO.JSON_PROPERTY_ENABLE_ELASTIC_DISK,
        NewClusterDTO.JSON_PROPERTY_INSTANCE_POOL_ID,
        NewClusterDTO.JSON_PROPERTY_DRIVER_INSTANCE_POOL_ID,
})
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaJerseyServerCodegen", date = "2022-03-09T23:53:34.566-08:00[America/Los_Angeles]")
public class NewClusterDTO {
    public static final String JSON_PROPERTY_NUM_WORKERS = "num_workers";
    public static final String JSON_PROPERTY_AUTOSCALE = "autoscale";
    public static final String JSON_PROPERTY_SPARK_VERSION = "spark_version";
    public static final String JSON_PROPERTY_SPARK_CONF = "spark_conf";
    public static final String JSON_PROPERTY_AZURE_ATTRIBUTES = "azure_attributes";
    public static final String JSON_PROPERTY_NODE_TYPE_ID = "node_type_id";
    public static final String JSON_PROPERTY_DRIVER_NODE_TYPE_ID = "driver_node_type_id";
    public static final String JSON_PROPERTY_CUSTOM_TAGS = "custom_tags";
    public static final String JSON_PROPERTY_CLUSTER_LOG_CONF = "cluster_log_conf";
    public static final String JSON_PROPERTY_INIT_SCRIPTS = "init_scripts";
    public static final String JSON_PROPERTY_SPARK_ENV_VARS = "spark_env_vars";
    public static final String JSON_PROPERTY_ENABLE_ELASTIC_DISK = "enable_elastic_disk";
    public static final String JSON_PROPERTY_INSTANCE_POOL_ID = "instance_pool_id";
    public static final String JSON_PROPERTY_DRIVER_INSTANCE_POOL_ID = "driver_instance_pool_id";
    private static final String JSON_PROPERTY_AWS_ATTRIBUTES = "aws_attributes";
    @JsonProperty(JSON_PROPERTY_NUM_WORKERS)
    private Integer numWorkers;
    @JsonProperty(JSON_PROPERTY_AUTOSCALE)
    private AutoScaleDTO autoscale;
    @JsonProperty(JSON_PROPERTY_SPARK_VERSION)
    private String sparkVersion;
    @JsonProperty(JSON_PROPERTY_SPARK_CONF)
    private Map<String, String> sparkConf = null;
    @JsonProperty(JSON_PROPERTY_AZURE_ATTRIBUTES)
    private AzureAttributes azureAttributes;
    @JsonProperty(JSON_PROPERTY_AWS_ATTRIBUTES)
    private AwsAttributesDTO awsAttributes;
    @JsonProperty(JSON_PROPERTY_NODE_TYPE_ID)
    private String nodeTypeId;
    @JsonProperty(JSON_PROPERTY_DRIVER_NODE_TYPE_ID)
    private String driverNodeTypeId;
    @JsonProperty(JSON_PROPERTY_CUSTOM_TAGS)
    private Object customTags = null;
    @JsonProperty(JSON_PROPERTY_CLUSTER_LOG_CONF)
    private ClusterLogConfDTO clusterLogConf;
    @JsonProperty(JSON_PROPERTY_INIT_SCRIPTS)
    private List<InitScriptInfoDTO> initScripts = null;
    @JsonProperty(JSON_PROPERTY_SPARK_ENV_VARS)
    private Map<String, String> sparkEnvVars = null;
    @JsonProperty(JSON_PROPERTY_ENABLE_ELASTIC_DISK)
    private Boolean enableElasticDisk;
    @JsonProperty(JSON_PROPERTY_INSTANCE_POOL_ID)
    private String instancePoolId;
    @JsonProperty(JSON_PROPERTY_DRIVER_INSTANCE_POOL_ID)
    private String driverInstancePoolId;

    public NewClusterDTO numWorkers(Integer numWorkers) {
        this.numWorkers = numWorkers;
        return this;
    }

    /**
     * If num_workers, number of worker nodes that this cluster must have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For example, if a cluster is resized from 5 to 10 workers, this field immediately updates to reflect the target size of 10 workers, whereas the workers listed in &#x60;spark_info&#x60; gradually increase from 5 to 10 as the new nodes are provisioned.
     *
     * @return numWorkers
     **/
    @JsonProperty(value = "num_workers")
    public Integer getNumWorkers() {
        return numWorkers;
    }

    public void setNumWorkers(Integer numWorkers) {
        this.numWorkers = numWorkers;
    }

    public NewClusterDTO autoscale(AutoScaleDTO autoscale) {
        this.autoscale = autoscale;
        return this;
    }

    /**
     * Get autoscale
     *
     * @return autoscale
     **/
    @JsonProperty(value = "autoscale")
    public AutoScaleDTO getAutoscale() {
        return autoscale;
    }

    public void setAutoscale(AutoScaleDTO autoscale) {
        this.autoscale = autoscale;
    }

    public NewClusterDTO sparkVersion(String sparkVersion) {
        this.sparkVersion = sparkVersion;
        return this;
    }

    /**
     * The Spark version of the cluster. A list of available Spark versions can be retrieved by using the [Runtime versions](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/clusters#runtime-versions) API call. This field is required.
     *
     * @return sparkVersion
     **/
    @JsonProperty(value = "spark_version")

    public String getSparkVersion() {
        return sparkVersion;
    }

    public void setSparkVersion(String sparkVersion) {
        this.sparkVersion = sparkVersion;
    }

    public NewClusterDTO sparkConf(Map<String, String> sparkConf) {
        this.sparkConf = sparkConf;
        return this;
    }

    public NewClusterDTO putSparkConfItem(String key, String sparkConfItem) {
        if (this.sparkConf == null) {
            this.sparkConf = new HashMap<String, String>();
        }
        this.sparkConf.put(key, sparkConfItem);
        return this;
    }

    /**
     * An arbitrary object where the object key is a configuration propery name and the value is a configuration property value.
     *
     * @return sparkConf
     **/
    @JsonProperty(value = "spark_conf")

    public Map<String, String> getSparkConf() {
        return sparkConf;
    }

    public void setSparkConf(Map<String, String> sparkConf) {
        this.sparkConf = sparkConf;
    }

    public NewClusterDTO azureAttributes(AzureAttributes azureAttributes) {
        this.azureAttributes = azureAttributes;
        return this;
    }

    /**
     * Get azureAttributes
     *
     * @return azureAttributes
     **/
    @JsonProperty(value = "azure_attributes")
    public AzureAttributes getAzureAttributes() {
        return azureAttributes;
    }

    public void setAzureAttributes(AzureAttributes azureAttributes) {
        this.azureAttributes = azureAttributes;
    }

    @JsonProperty(value = JSON_PROPERTY_AWS_ATTRIBUTES)
    public AwsAttributesDTO getAwsAttributes() {
        return awsAttributes;
    }

    public void setAwsAttributes(AwsAttributesDTO awsAttributes) {
        this.awsAttributes = awsAttributes;
    }

    public NewClusterDTO nodeTypeId(String nodeTypeId) {
        this.nodeTypeId = nodeTypeId;
        return this;
    }

    /**
     * This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the [List node types](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/clusters#list-node-types) API call. This field is required.
     *
     * @return nodeTypeId
     **/
    @JsonProperty(value = "node_type_id")

    public String getNodeTypeId() {
        return nodeTypeId;
    }

    public void setNodeTypeId(String nodeTypeId) {
        this.nodeTypeId = nodeTypeId;
    }

    public NewClusterDTO driverNodeTypeId(String driverNodeTypeId) {
        this.driverNodeTypeId = driverNodeTypeId;
        return this;
    }

    /**
     * The node type of the Spark driver. This field is optional; if unset, the driver node type is set as the same value as &#x60;node_type_id&#x60; defined above.
     *
     * @return driverNodeTypeId
     **/
    @JsonProperty(value = "driver_node_type_id")

    public String getDriverNodeTypeId() {
        return driverNodeTypeId;
    }

    public void setDriverNodeTypeId(String driverNodeTypeId) {
        this.driverNodeTypeId = driverNodeTypeId;
    }

    public NewClusterDTO customTags(Object customTags) {
        this.customTags = customTags;
        return this;
    }

    /**
     * Get customTags
     *
     * @return customTags
     **/
    @JsonProperty(value = "custom_tags")

    public Object getCustomTags() {
        return customTags;
    }

    public void setCustomTags(Object customTags) {
        this.customTags = customTags;
    }

    public NewClusterDTO clusterLogConf(ClusterLogConfDTO clusterLogConf) {
        this.clusterLogConf = clusterLogConf;
        return this;
    }

    /**
     * Get clusterLogConf
     *
     * @return clusterLogConf
     **/
    @JsonProperty(value = "cluster_log_conf")

    public ClusterLogConfDTO getClusterLogConf() {
        return clusterLogConf;
    }

    public void setClusterLogConf(ClusterLogConfDTO clusterLogConf) {
        this.clusterLogConf = clusterLogConf;
    }

    public NewClusterDTO initScripts(List<InitScriptInfoDTO> initScripts) {
        this.initScripts = initScripts;
        return this;
    }

    public NewClusterDTO addInitScriptsItem(InitScriptInfoDTO initScriptsItem) {
        if (this.initScripts == null) {
            this.initScripts = new ArrayList<InitScriptInfoDTO>();
        }
        this.initScripts.add(initScriptsItem);
        return this;
    }

    /**
     * The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If &#x60;cluster_log_conf&#x60; is specified, init script logs are sent to &#x60;&lt;destination&gt;/&lt;cluster-id&gt;/init_scripts&#x60;.
     *
     * @return initScripts
     **/
    @JsonProperty(value = "init_scripts")
    public List<InitScriptInfoDTO> getInitScripts() {
        return initScripts;
    }

    public void setInitScripts(List<InitScriptInfoDTO> initScripts) {
        this.initScripts = initScripts;
    }

    public NewClusterDTO sparkEnvVars(Map<String, String> sparkEnvVars) {
        this.sparkEnvVars = sparkEnvVars;
        return this;
    }

    public NewClusterDTO putSparkEnvVarsItem(String key, String sparkEnvVarsItem) {
        if (this.sparkEnvVars == null) {
            this.sparkEnvVars = new HashMap<String, String>();
        }
        this.sparkEnvVars.put(key, sparkEnvVarsItem);
        return this;
    }

    /**
     * An arbitrary object where the object key is an environment variable name and the value is an environment variable value.
     *
     * @return sparkEnvVars
     **/
    @JsonProperty(value = "spark_env_vars")
    public Map<String, String> getSparkEnvVars() {
        return sparkEnvVars;
    }

    public void setSparkEnvVars(Map<String, String> sparkEnvVars) {
        this.sparkEnvVars = sparkEnvVars;
    }

    public NewClusterDTO enableElasticDisk(Boolean enableElasticDisk) {
        this.enableElasticDisk = enableElasticDisk;
        return this;
    }

    /**
     * Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. Refer to [Autoscaling local storage](https://docs.microsoft.com/azure/databricks/clusters/configure#autoscaling-local-storage-azure) for details.
     *
     * @return enableElasticDisk
     **/
    @JsonProperty(value = "enable_elastic_disk")
    public Boolean getEnableElasticDisk() {
        return enableElasticDisk;
    }

    public void setEnableElasticDisk(Boolean enableElasticDisk) {
        this.enableElasticDisk = enableElasticDisk;
    }

    public NewClusterDTO instancePoolId(String instancePoolId) {
        this.instancePoolId = instancePoolId;
        return this;
    }

    /**
     * The optional ID of the instance pool to use for cluster nodes. If &#x60;driver_instance_pool_id&#x60; is present, &#x60;instance_pool_id&#x60; is used for worker nodes only. Otherwise, it is used for both the driver node and worker nodes. Refer to [Instance Pools API](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/instance-pools) for details.
     *
     * @return instancePoolId
     **/
    @JsonProperty(value = "instance_pool_id")
    public String getInstancePoolId() {
        return instancePoolId;
    }

    public void setInstancePoolId(String instancePoolId) {
        this.instancePoolId = instancePoolId;
    }

    public NewClusterDTO driverInstancePoolId(String driverInstancePoolId) {
        this.driverInstancePoolId = driverInstancePoolId;
        return this;
    }

    /**
     * The optional ID of the instance pool to use for the driver node. You must also specify instance_pool_id. Refer to [Instance Pools API 2.0](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/instance-pools) for details.
     *
     * @return instancePoolId
     **/
    @JsonProperty(value = "driver_instance_pool_id")
    public String getDriverInstancePoolId() {
        return driverInstancePoolId;
    }

    public void setDriverInstancePoolId(String driverInstancePoolId) {
        this.driverInstancePoolId = driverInstancePoolId;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) {
            return true;
        }
        if (o == null || getClass() != o.getClass()) {
            return false;
        }
        NewClusterDTO newCluster = (NewClusterDTO) o;
        return Objects.equals(this.numWorkers, newCluster.numWorkers) &&
                Objects.equals(this.autoscale, newCluster.autoscale) &&
                Objects.equals(this.sparkVersion, newCluster.sparkVersion) &&
                Objects.equals(this.sparkConf, newCluster.sparkConf) &&
                Objects.equals(this.azureAttributes, newCluster.azureAttributes) &&
                Objects.equals(this.nodeTypeId, newCluster.nodeTypeId) &&
                Objects.equals(this.driverNodeTypeId, newCluster.driverNodeTypeId) &&
                Objects.equals(this.customTags, newCluster.customTags) &&
                Objects.equals(this.clusterLogConf, newCluster.clusterLogConf) &&
                Objects.equals(this.initScripts, newCluster.initScripts) &&
                Objects.equals(this.sparkEnvVars, newCluster.sparkEnvVars) &&
                Objects.equals(this.enableElasticDisk, newCluster.enableElasticDisk) &&
                Objects.equals(this.instancePoolId, newCluster.instancePoolId);
    }

    @Override
    public int hashCode() {
        return Objects.hash(numWorkers, autoscale, sparkVersion, sparkConf, azureAttributes, nodeTypeId, driverNodeTypeId, customTags, clusterLogConf, initScripts, sparkEnvVars, enableElasticDisk, instancePoolId);
    }


    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("class NewClusterDTO {\n");

        sb.append("    numWorkers: ").append(toIndentedString(numWorkers)).append("\n");
        sb.append("    autoscale: ").append(toIndentedString(autoscale)).append("\n");
        sb.append("    sparkVersion: ").append(toIndentedString(sparkVersion)).append("\n");
        sb.append("    sparkConf: ").append(toIndentedString(sparkConf)).append("\n");
        sb.append("    azureAttributes: ").append(toIndentedString(azureAttributes)).append("\n");
        sb.append("    nodeTypeId: ").append(toIndentedString(nodeTypeId)).append("\n");
        sb.append("    driverNodeTypeId: ").append(toIndentedString(driverNodeTypeId)).append("\n");
        sb.append("    customTags: ").append(toIndentedString(customTags)).append("\n");
        sb.append("    clusterLogConf: ").append(toIndentedString(clusterLogConf)).append("\n");
        sb.append("    initScripts: ").append(toIndentedString(initScripts)).append("\n");
        sb.append("    sparkEnvVars: ").append(toIndentedString(sparkEnvVars)).append("\n");
        sb.append("    enableElasticDisk: ").append(toIndentedString(enableElasticDisk)).append("\n");
        sb.append("    instancePoolId: ").append(toIndentedString(instancePoolId)).append("\n");
        sb.append("}");
        return sb.toString();
    }

    /**
     * Convert the given object to string with each line indented by 4 spaces
     * (except the first line).
     */
    private String toIndentedString(Object o) {
        if (o == null) {
            return "null";
        }
        return o.toString().replace("\n", "\n    ");
    }
}

